{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682f35f1",
   "metadata": {},
   "source": [
    "# CSC 8614 - Language Models\n",
    "## CI3 - Parameter-Efficient Fine-Tuning with LoRA\n",
    "\n",
    "This TP builds upon the GPT architecture we have previously explored. \n",
    "\n",
    "We will implement Low-Rank Adaptation (LoRA) from scratch and inject it into our pre-existing GPTModel.\n",
    "\n",
    "Objectives:\n",
    "- Implement the mathematical formulation of LoRA.\n",
    "- Create a wrapper to convert standard Linear layers into LoRA-compatible layers.\n",
    "- Inject these layers into a pre-trained GPT model.\n",
    "- Verify that only a fraction of parameters are trainable.\n",
    "- Fine-tune the model with LoRA\n",
    "\n",
    "Some of this code comes from the book _Build a Large Language Model (From Scratch)_, by Sebastian Raschka, and its [official github repository](https://github.com/rasbt/LLMs-from-scratch).\n",
    "\n",
    "This TP will be done in this notebook, and requires some additional files (available from the course website). \n",
    "You will have to fill the missing portions of code, and perform some additional experiments by testing different parameters.\n",
    "\n",
    "Working on this TP:\n",
    "- The easiest way is probably to work directly on the notebook, using jupyter notebook or visual studio code. An alternative is also to use Google colab.\n",
    "- You should be able to run everything on your machine, but you can connect to the GPUs if needed.\n",
    "- **NOTE**: run the cells in the correct order, otherwise you might get errors due to inconsintencies.\n",
    "\n",
    "Some files are required, and are available on the course website and/or github repo:\n",
    "- `requirements.txt`\n",
    "- `gpt_utils.py`\n",
    "\n",
    "\n",
    "## About the report\n",
    "You will have to return this notebook (completed), as well as a mini-report (`TP3/rapport.md`).\n",
    "\n",
    "The notebook and report shall be submitted via a GitHub repository, similarly to what you did for the previous sessions (remember to use a different folder: `TP3`).\n",
    "For the notebook, it is sufficient to complete the code and submit the final version.\n",
    "\n",
    "For the mini-report, you have to answer the questions asked in this notebook, and discuss some of your findings as requested.\n",
    "Same as in the previous sessions:\n",
    "- You must include: short answers, observed results (copies of outputs), requested screenshots, and a brief interpretation.\n",
    "- Do not paste entire pages: be concise and select the relevant elements.\n",
    "\n",
    "Reproducibility: \n",
    "- fix a random seed and write it in the report\n",
    "- indicate in the report the specific python version OS, and the library versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8ead2",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "Install the requirements.\n",
    "\n",
    "**Note**: if you use the same virtual environment as last time, you will not have to reinstall everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347af8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.9.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: tiktoken==0.12.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
      "Requirement already satisfied: pandas==2.3.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: matplotlib==3.10.8 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.10.8)\n",
      "Requirement already satisfied: tensorflow==2.20.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (2.20.0)\n",
      "Requirement already satisfied: jupyterlab==4.5.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (4.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from torch==2.9.1->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tiktoken==0.12.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tqdm==4.67.1->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from pandas==2.3.3->-r requirements.txt (line 4)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from matplotlib==3.10.8->-r requirements.txt (line 5)) (3.3.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (6.33.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorflow==2.20.0->-r requirements.txt (line 6)) (0.5.4)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.28.1)\n",
      "Requirement already satisfied: ipykernel!=6.30.0,>=6.5.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.1.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.9.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.5.4)\n",
      "Requirement already satisfied: traitlets in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (8.8.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.23.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (27.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.13.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.26.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.12.0->-r requirements.txt (line 2)) (2.6.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (3.1.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.45.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.8.19)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (9.9.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (7.2.1)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.8.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jinja2->torch==2.9.1->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.30.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-core->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.5.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.0.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (25.10.0)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (4.14.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.21.2)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel!=6.30.0,>=6.5.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.23)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (2.8.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab==4.5.1->-r requirements.txt (line 7)) (1.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\desktop\\nlp_tp1\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow==2.20.0->-r requirements.txt (line 6)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed979fc7",
   "metadata": {},
   "source": [
    "## Background & Mathematical Formulation\n",
    "\n",
    "Fine-tuning Large Language Models (LLMs) updates all model parameters, which is computationally expensive. LoRA freezes the pre-trained weights and injects trainable rank decomposition matrices.\n",
    "\n",
    "Given a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, LoRA constrains the update $\\Delta W$ by representing it with a low-rank decomposition:\n",
    "\n",
    "$$W_0 + \\Delta W = W_0 + B A$$\n",
    "\n",
    "Where:\n",
    "- $B \\in \\mathbb{R}^{d_{out} \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{in}}$\n",
    "- $r \\ll \\min(d_{in}, d_{out})$ is the rank.\n",
    "\n",
    "The Forward Pass:\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} (B A x)$$\n",
    "\n",
    "- $\\alpha$ is a scaling constant.\n",
    "- $A$ is initialized with random Gaussian values.\n",
    "- $B$ is initialized with zeros (so training starts with no changes to the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbec02b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and our provided GPT utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15a71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from gpt_utils import GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685e3ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a699e2cbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_STATE = 123 \n",
    "torch.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a222d",
   "metadata": {},
   "source": [
    "## Implementing the LoRA Layer\n",
    "\n",
    "In this exercise, you will create the LoRALayer module. This module computes the $\\Delta Wx$ term (the branch on the right side of the diagram seen during the lecture).\n",
    "\n",
    "### **Exercise 1**: Define the LoRA Module\n",
    "\n",
    "Requirements:\n",
    "1. Define dimensions for parameters A and B based on in_dim, out_dim, and rank.\n",
    "2. Initialize A with kaiming_uniform_ (or small random normal).\n",
    "3. Initialize B with zeros.\n",
    "4. Implement the forward pass including the scaling factor $\\frac{\\alpha}{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be539ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank  # alpha/r\n",
    "\n",
    "        # A: rank x in_dim, B: out_dim x rank\n",
    "        self.A = nn.Parameter(torch.empty(rank, in_dim))\n",
    "        self.B = nn.Parameter(torch.empty(out_dim, rank))\n",
    "\n",
    "        # Init\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., in_dim)\n",
    "        # (..., rank)\n",
    "        x_a = torch.matmul(x, self.A.t())\n",
    "        # (..., out_dim)\n",
    "        x_ab = torch.matmul(x_a, self.B.t())\n",
    "        return self.scaling * x_ab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767c887",
   "metadata": {},
   "source": [
    "## Wrapping Linear Layers\n",
    "\n",
    "We rarely replace the layer entirely; instead, we want a layer that holds both the frozen original weights and the new LoRA weights.\n",
    "\n",
    "### **Exercise 2**: The LinearWithLoRA Wrapper\n",
    "\n",
    "Requirements:\n",
    "1. Store the original linear layer.\n",
    "2. Create a self.lora instance using the dimensions of the original linear layer.\n",
    "3. In forward, add the output of the original layer to the output of the LoRA layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d268ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed: Wrapper acts identically to original layer at initialization.\n"
     ]
    }
   ],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear_layer, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        self.lora = LoRALayer(\n",
    "            in_dim=linear_layer.in_features,\n",
    "            out_dim=linear_layer.out_features,\n",
    "            rank=rank,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# --- Sanity Check ---\n",
    "input_x = torch.randn(1, 128)\n",
    "original_layer = nn.Linear(128, 64)\n",
    "lora_wrapped = LinearWithLoRA(original_layer, rank=4, alpha=8)\n",
    "\n",
    "assert torch.allclose(original_layer(input_x), lora_wrapped(input_x))\n",
    "print(\"Test Passed: Wrapper acts identically to original layer at initialization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201146a1",
   "metadata": {},
   "source": [
    "## Injecting LoRA into GPTModel\n",
    "\n",
    "Now we need to modify our existing GPTModel. We cannot manually rewrite the class. Instead, we will iterate through the model's modules and replace specific layers dynamically.\n",
    "\n",
    "In GPTModel, the transformer blocks are stored in *self.trf_blocks*. Inside those, we have attention mechanisms (att) containing W_query, W_key, W_value, or a combined c_attn.\n",
    "\n",
    "Note: For this lab, to keep it simple, we will replace all nn.Linear layers except the final output head.\n",
    "\n",
    "### **Exercise 3**: Recursive Model Modification\n",
    "\n",
    "Requirements:\n",
    "1. Iterate through named children of the model.\n",
    "2. If a module is nn.Linear, wrap it in LinearWithLoRA.\n",
    "3. Important: Skip the final output layer (often named out_head or similar in gpt_utils), as we usually don't want to reduce the rank of the vocabulary projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    \"\"\"\n",
    "    Recursively replaces nn.Linear with LinearWithLoRA, skipping out_head.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if name == \"out_head\":\n",
    "                continue\n",
    "\n",
    "            new_layer = LinearWithLoRA(module, rank=rank, alpha=alpha)\n",
    "            setattr(model, name, new_layer)\n",
    "\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5f1d3",
   "metadata": {},
   "source": [
    "## Freeze & Verify\n",
    "\n",
    "We have injected the layers, but currently, everything is still trainable. We must freeze the original weights.\n",
    "\n",
    "### **Exercise 4**: Freezing and Counting Parameters\n",
    "Requirements:\n",
    "1. Set requires_grad = False for all parameters.\n",
    "2. Iterate through the model; if a layer is LinearWithLoRA, unfreeze self.lora.A and self.lora.B.\n",
    "3. Calculate the ratio of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5764f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_and_activate_lora(model):\n",
    "    # 1) Freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 2) Unfreeze only LoRA A and B\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab6916",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Let's download and initialize the model from gpt_utils, then apply our LoRA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73aae7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 54.9kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 818kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 42.0kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [04:28<00:00, 1.85MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 3.11MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 488kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 325kiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights downloaded and loaded into memory.\n",
      "Original Model Structure (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Model Structure After LoRA (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_key): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_value): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (out_proj): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (1): GELU()\n",
      "      (2): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Parameter Count:\n",
      "trainable params: 1,327,104 || all params: 164,364,288 || trainable%: 0.81%\n"
     ]
    }
   ],
   "source": [
    "from gpt_utils import GPTModel, download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2_weights\")\n",
    "print(\"Weights downloaded and loaded into memory.\")\n",
    "\n",
    "model_config = {\n",
    "    \"vocab_size\": settings[\"n_vocab\"],\n",
    "    \"context_length\": settings[\"n_ctx\"],\n",
    "    \"emb_dim\": settings[\"n_embd\"],\n",
    "    \"n_heads\": settings[\"n_head\"],\n",
    "    \"n_layers\": settings[\"n_layer\"],\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True,\n",
    "}\n",
    "\n",
    "model = GPTModel(model_config)\n",
    "load_weights_into_gpt(model, params)\n",
    "\n",
    "print(\"Original Model Structure (Truncated):\")\n",
    "print(model.trf_blocks[0])\n",
    "\n",
    "replace_linear_with_lora(model, rank=8, alpha=16)\n",
    "freeze_and_activate_lora(model)\n",
    "\n",
    "print(\"\\nModel Structure After LoRA (Truncated):\")\n",
    "print(model.trf_blocks[0])\n",
    "\n",
    "print(\"\\nParameter Count:\")\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26ffba",
   "metadata": {},
   "source": [
    "Now, we call all the methods we have defined above, and put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4001e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Structure (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_key): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_value): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (out_proj): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (1): GELU()\n",
      "      (2): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Model Structure After LoRA (Truncated):\n",
      "TransformerBlock(\n",
      "  (att): MultiHeadAttention(\n",
      "    (W_query): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_key): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (W_value): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (out_proj): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ff): FeedForward(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearWithLoRA(\n",
      "        (linear): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (1): GELU()\n",
      "      (2): LinearWithLoRA(\n",
      "        (linear): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm1): LayerNorm()\n",
      "  (norm2): LayerNorm()\n",
      "  (drop_resid): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Parameter Count:\n",
      "trainable params: 2,654,208 || all params: 165,691,392 || trainable%: 1.60%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "print(\"Original Model Structure (Truncated):\")\n",
    "print(model.trf_blocks[0]) # Print first block to see standard Linear layers\n",
    "\n",
    "# Apply LoRA Replacement\n",
    "# Rank 8, Alpha 16 (Alpha is usually set to 2x Rank as a rule of thumb)\n",
    "replace_linear_with_lora(model, rank=8, alpha=16)\n",
    "\n",
    "# 3. Freeze Weights\n",
    "freeze_and_activate_lora(model)\n",
    "\n",
    "# 4. Check Results\n",
    "print(\"\\nModel Structure After LoRA (Truncated):\")\n",
    "print(model.trf_blocks[0])\n",
    "\n",
    "print(\"\\nParameter Count:\")\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a264553",
   "metadata": {},
   "source": [
    "**Question 1:** Do you see any difference between \"Original Model Structure (Truncated)\" and \"Model Structure After LoRA (Truncated)\"? Do you see the LinearWithLoRA you have defined above?\n",
    "\n",
    "**Question 2:** What is the number of trainable parameters, all parameters, and the fraction of trainable parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e97c6",
   "metadata": {},
   "source": [
    "## Training Loop Verification\n",
    "\n",
    "Finally, let's prove that gradients are only generated for the specific LoRA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8076639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Verification:\n",
      "Parameters with gradients: 288\n",
      "Frozen parameters correctly without gradients: 197\n",
      "SUCCESS: Gradients are flowing correctly only into LoRA parameters.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "dummy_input = torch.randint(0, 1000, (batch_size, 256))\n",
    "dummy_target = torch.randint(0, 1000, (batch_size, 256))\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=1e-3)\n",
    "\n",
    "logits = model(dummy_input)\n",
    "loss = torch.nn.functional.cross_entropy(\n",
    "    logits.view(-1, model_config[\"vocab_size\"]),\n",
    "    dummy_target.view(-1)\n",
    ")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradient Verification:\")\n",
    "grads_found = 0\n",
    "grads_missing = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if param.grad is not None:\n",
    "            grads_found += 1\n",
    "        else:\n",
    "            print(f\"WARNING: Trainable parameter {name} has no gradient.\")\n",
    "    else:\n",
    "        if param.grad is not None:\n",
    "            print(f\"ERROR: Frozen parameter {name} has a gradient!\")\n",
    "        else:\n",
    "            grads_missing += 1\n",
    "\n",
    "print(f\"Parameters with gradients: {grads_found}\")\n",
    "print(f\"Frozen parameters correctly without gradients: {grads_missing}\")\n",
    "\n",
    "if grads_found > 0 and grads_missing > 0:\n",
    "    print(\"SUCCESS: Gradients are flowing correctly only into LoRA parameters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff671a8",
   "metadata": {},
   "source": [
    "## SPAM Classification\n",
    "\n",
    "Let's now work the Spam Classification task again, but this time using the LoRA-adapted model. \n",
    "This follows the example from the previous session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933e138",
   "metadata": {},
   "source": [
    "### Download and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beff640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded and extracted.\n",
      "Loaded 5572 examples.\n",
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Balanced Dataset size: 1494\n",
      "Size of train_df: 1195\n",
      "Size of test_df: 299\n",
      "Distribution of labels in the dataset:\n",
      "train:\n",
      "label\n",
      "1    598\n",
      "0    597\n",
      "Name: count, dtype: int64\n",
      "test:\n",
      "label\n",
      "0    150\n",
      "1    149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = os.path.join(extracted_path, \"SMSSpamCollection\")\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "    print(\"Dataset downloaded and extracted.\")\n",
    "\n",
    "# Load into DataFrame\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "print(f\"Loaded {len(df)} examples.\")\n",
    "print(df.head())\n",
    "\n",
    "# Class Balancing (primarily for speed in the lab)\n",
    "spam_df = df[df[\"label\"] == \"spam\"]\n",
    "ham_df = df[df[\"label\"] == \"ham\"].sample(len(spam_df), random_state=RANDOM_STATE)\n",
    "df = pd.concat([spam_df, ham_df]).reset_index(drop=True)\n",
    "\n",
    "# Map labels to integers\n",
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "print(f\"Balanced Dataset size: {len(df)}\")\n",
    "\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "print(f\"Size of train_df: {len(train_df)}\")\n",
    "print(f\"Size of test_df: {len(test_df)}\")\n",
    "\n",
    "print(\"Distribution of labels in the dataset:\")\n",
    "print(\"train:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(\"test:\")\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb81f48",
   "metadata": {},
   "source": [
    "Define Dataset and DataLoader, similarly to previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b33971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=256):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        # Truncate\n",
    "        encoded = encoded[:self.max_length]\n",
    "        \n",
    "        # Pad (GPT-2 usually uses <|endoftext|> as padding)\n",
    "        pad_len = self.max_length - len(encoded)\n",
    "        encoded = encoded + [50256] * pad_len # 50256 is <|endoftext|> in GPT2\n",
    "        \n",
    "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Create Loaders\n",
    "train_loader = DataLoader(SpamDataset(train_df, tokenizer), batch_size=8, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(SpamDataset(test_df, tokenizer), batch_size=8, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b9dc8",
   "metadata": {},
   "source": [
    "We now modify the Model for Classification, replacing the final layer (out_head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e686056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Output Head: Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Check input dimension of the current head\n",
    "hidden_dim = model.out_head.in_features\n",
    "\n",
    "# Replace the head\n",
    "model.out_head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "# Move model to device (if using GPU, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"New Output Head:\", model.out_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4b703",
   "metadata": {},
   "source": [
    "We previously froze everything except LoRA. Now we added a new head, let's make it unfrozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6889edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,655,746 || all params: 127,095,554 || trainable%: 2.09%\n"
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def set_classification_trainable(model):\n",
    "    # Ensure LoRA layers are trainable (A and B)\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LinearWithLoRA):\n",
    "            module.lora.A.requires_grad = True\n",
    "            module.lora.B.requires_grad = True\n",
    "    \n",
    "    # Ensure the new classification head is trainable\n",
    "    for param in model.out_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "set_classification_trainable(model)\n",
    "\n",
    "# Verify count again\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dec47a",
   "metadata": {},
   "source": [
    "**Question 3:** Check the number (and fraction) of trainable parameters, and compare it with the one above. Do you see any differences? Can you describe them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5f0ac",
   "metadata": {},
   "source": [
    "The Training Loop\n",
    "Context: Standard PyTorch loop. Note that GPT models output [batch, seq_len, hidden]. For classification, we usually take the hidden state of the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c03d3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "import time\n",
    "\n",
    "def train_classifier(model, loader, optimizer, device, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            # The model outputs (batch, seq_len, num_classes)\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select the last token for classification\n",
    "            # last_token_logits = logits[:, -1, :]  \n",
    "            # NOTE: this (the line above) was an error in the code provided with the previous lab: it was using the last token (which is most often PAD), \n",
    "            #   not the last non padding token. \n",
    "            # Select the last NON-PADDING token\n",
    "            #   Create a mask (1 for real tokens, 0 for PAD); 50256 is the PAD token ID\n",
    "            mask = (inputs != 50256)\n",
    "            \n",
    "            # Find the index of the last real token\n",
    "            #    Summing the mask gives the length. Subtract 1 for 0-based index.\n",
    "            #    .clamp(min=0) prevents errors if a sequence is empty (unlikely)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            \n",
    "            # Select the logits at those specific indices\n",
    "            #    We use torch.arange for the batch dimension\n",
    "            batch_indices = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_indices, last_idx, :]\n",
    "            \n",
    "            loss = torch.nn.functional.cross_entropy(last_token_logits, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy for monitoring\n",
    "            preds = torch.argmax(last_token_logits, dim=-1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        acc = correct / total * 100\n",
    "        print(f\"Epoch {epoch+1} Finished | Avg Loss: {total_loss/len(loader):.4f} | Acc: {acc:.2f}% | Time: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d347648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch 0 | Loss: 1.5386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m optimizer = torch.optim.AdamW(\n\u001b[32m      5\u001b[39m     [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad], \n\u001b[32m      6\u001b[39m     lr=\u001b[32m5e-4\u001b[39m    \u001b[38;5;66;03m# TODO: Potentially test with different learning rates\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run Training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TODO: Potentially test with different numbers of epochs\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_classifier\u001b[39m\u001b[34m(model, loader, optimizer, device, epochs)\u001b[39m\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Forward Pass\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# The model outputs (batch, seq_len, num_classes)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Select the last token for classification\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# last_token_logits = logits[:, -1, :]  \u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# NOTE: this (the line above) was an error in the code provided with the previous lab: it was using the last token (which is most often PAD), \u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#   not the last non padding token. \u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Select the last NON-PADDING token\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#   Create a mask (1 for real tokens, 0 for PAD); 50256 is the PAD token ID\u001b[39;00m\n\u001b[32m     29\u001b[39m mask = (inputs != \u001b[32m50256\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\TP3\\gpt_utils.py:227\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m    225\u001b[39m x = tok_embeds + pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[32m    226\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_emb(x)\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_norm(x)\n\u001b[32m    229\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.out_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\TP3\\gpt_utils.py:186\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    184\u001b[39m shortcut = x\n\u001b[32m    185\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[32m    187\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_resid(x)\n\u001b[32m    188\u001b[39m x = x + shortcut  \u001b[38;5;66;03m# Add the original input back\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\TP3\\gpt_utils.py:114\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    111\u001b[39m attn_scores.masked_fill_(mask_bool, -torch.inf)\n\u001b[32m    113\u001b[39m attn_weights = torch.softmax(attn_scores / keys.shape[-\u001b[32m1\u001b[39m]**\u001b[32m0.5\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Shape: (b, num_tokens, num_heads, head_dim)\u001b[39;00m\n\u001b[32m    117\u001b[39m context_vec = (attn_weights @ values).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\NLP_TP1\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Setup Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], \n",
    "    lr=5e-4    # TODO: Potentially test with different learning rates\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "train_classifier(model, train_loader, optimizer, device, epochs=1)  # TODO: Potentially test with different numbers of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02917255",
   "metadata": {},
   "source": [
    "**Question 4:** Can you describe the trend of the loss, and the final accuracy. Is it reasonable considering the task at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e655eb",
   "metadata": {},
   "source": [
    "We can now test the accuracy on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd866e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def evaluate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(inputs)\n",
    "            \n",
    "            # Select last \"real\" token logits\n",
    "            mask = (inputs != 50256)\n",
    "            last_idx = (mask.sum(dim=1) - 1).clamp(min=0)\n",
    "            batch_idx = torch.arange(inputs.size(0), device=device)\n",
    "            last_token_logits = logits[batch_idx, last_idx, :]\n",
    "            # Select last token logits (same logic as training)\n",
    "            # last_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(last_token_logits, dim=-1)\n",
    "            \n",
    "            # Compare with targets\n",
    "            correct += (predictions == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "# Run evaluation\n",
    "test_accuracy = evaluate_accuracy(model, test_loader, device)\n",
    "print(f\"Test Set Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713ca7",
   "metadata": {},
   "source": [
    "**Question 5:** How is the accuracy, and how does it compare to the Train set accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1b259",
   "metadata": {},
   "source": [
    "Finally, we can do a quick inference test, to see how the model classifies new texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- INSTRUCTOR CODE ---\n",
    "\n",
    "def classify_text(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded = encoded[:256] # Truncate\n",
    "    tensor_input = torch.tensor([encoded], dtype=torch.long).to(device) # Add batch dim\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor_input)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_token_logits, dim=-1)\n",
    "        pred_label = torch.argmax(probs, dim=-1).item()\n",
    "        \n",
    "    label_map = {0: \"HAM (Normal)\", 1: \"SPAM\"}\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Prediction: {label_map[pred_label]} (Confidence: {probs[0][pred_label]:.2f})\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the text you want to test.\n",
    "classify_text(\"There is a big cash prize for you, call immediately.\", model, tokenizer, device)\n",
    "classify_text(\"Hey, are we still meeting for lunch tomorrow?\", model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4d416",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
